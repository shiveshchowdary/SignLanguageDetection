{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoPWEQr2fd7C",
        "outputId": "59d89db4-9535-4d8a-864b-6da1de288b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'JointBERT'...\n",
            "remote: Enumerating objects: 332, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 332 (delta 105), reused 95 (delta 95), pack-reused 213\u001b[K\n",
            "Receiving objects: 100% (332/332), 487.19 KiB | 1.47 MiB/s, done.\n",
            "Resolving deltas: 100% (194/194), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/monologg/JointBERT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_name):\n",
        "  my_file  = open(file_name,'r')\n",
        "  data = my_file.read()\n",
        "  result = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "  return result"
      ],
      "metadata": {
        "id": "GadiLl8QhYwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = read_data('/content/JointBERT/data/snips/train/seq.in')\n",
        "train_label = read_data('/content/JointBERT/data/snips/train/label')"
      ],
      "metadata": {
        "id": "4Lim5QdAhrye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_text = read_data('/content/JointBERT/data/snips/dev/seq.in')\n",
        "dev_label = read_data('/content/JointBERT/data/snips/dev/label')"
      ],
      "metadata": {
        "id": "M_NRGfN8iMAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = read_data('/content/JointBERT/data/snips/test/seq.in')\n",
        "test_label = read_data('/content/JointBERT/data/snips/test/label')"
      ],
      "metadata": {
        "id": "Q_gG7PQGiTFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_numbers(lst):\n",
        "    result = []\n",
        "    for string in lst:\n",
        "        result.append(re.sub(r'\\d+', '', string))\n",
        "    return result"
      ],
      "metadata": {
        "id": "Uqh3U_SnibZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = remove_numbers(train_text)\n",
        "dev_text = remove_numbers(dev_text)\n",
        "test_text = remove_numbers(test_text)"
      ],
      "metadata": {
        "id": "RCvKh49bjRK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(train_label)):\n",
        "  if train_label[i]=='':\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71RL1EfOk0Sr",
        "outputId": "a002e422-3f5c-48c7-b44f-99fb936c2cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict = {\n",
        "    'text' : train_text[:-1],\n",
        "    'label' : train_label[:-1]\n",
        "}\n",
        "dev_dict = {\n",
        "    'text' : dev_text[:-1],\n",
        "    'label' : dev_label[:-1]\n",
        "}\n",
        "test_dict = {\n",
        "    'text' : test_text[:-1],\n",
        "    'label' : test_label[:-1]\n",
        "}"
      ],
      "metadata": {
        "id": "kDKlbmwSkRTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data = pd.DataFrame(train_dict)\n",
        "test_data = pd.DataFrame(test_dict)\n",
        "dev_data = pd.DataFrame(dev_dict)"
      ],
      "metadata": {
        "id": "fMF3qwcwjhLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vP6Nizdj1gW",
        "outputId": "d9969d29-81e5-43c0-c432-876ac40c9dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PlayMusic               1914\n",
              "GetWeather              1896\n",
              "BookRestaurant          1881\n",
              "RateBook                1876\n",
              "SearchScreeningEvent    1852\n",
              "SearchCreativeWork      1847\n",
              "AddToPlaylist           1818\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-bggd6ckr0e",
        "outputId": "38c0a57d-6a04-4f79-ee93-57094b920e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AddToPlaylist           124\n",
              "SearchScreeningEvent    107\n",
              "SearchCreativeWork      107\n",
              "GetWeather              104\n",
              "BookRestaurant           92\n",
              "PlayMusic                86\n",
              "RateBook                 80\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_data.label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Im-_7fklsL6",
        "outputId": "df92c89a-514b-4883-e046-11f6561924fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AddToPlaylist           100\n",
              "BookRestaurant          100\n",
              "GetWeather              100\n",
              "PlayMusic               100\n",
              "RateBook                100\n",
              "SearchCreativeWork      100\n",
              "SearchScreeningEvent    100\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFThABfOmQq4",
        "outputId": "169ce338-57d5-4415-c554-254236b08644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "EpVTKe-yl0qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cats = list(dev_data.label.unique())\n",
        "cat_dict = {}\n",
        "for i in range(len(cats)):\n",
        "  cat_dict[cats[i]]=i"
      ],
      "metadata": {
        "id": "0bytcGVmm4BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = list(train_data.text)[:200]\n",
        "train_labels = [cat_dict[i] for i in list(train_data.label)][:200]\n",
        "dev_texts = list(dev_data.text)\n",
        "dev_labels = [cat_dict[i] for i in list(dev_data.label)]\n",
        "test_texts = list(test_data.text)\n",
        "test_labels = [cat_dict[i] for i in list(test_data.label)]"
      ],
      "metadata": {
        "id": "xoM8trbpmOTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = tf.device(\"gpu\" if tf.config.list_physical_devices('GPU') else \"cpu\")\n",
        "\n",
        "# Define tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=7)\n",
        "\n",
        "# Define optimizer, loss function, and metrics\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "\n",
        "# Define data\n",
        "# Tokenize data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "dev_encodings = tokenizer(dev_texts, truncation=True, padding=True)\n",
        "\n",
        "# Convert data to TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).shuffle(len(train_texts)).batch(batch_size=16)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        ")).batch(batch_size=16)\n",
        "dev_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(dev_encodings),\n",
        "    dev_labels\n",
        ")).batch(batch_size=16)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# Train the model\n",
        "epochs = 15\n",
        "history = model.fit(train_dataset, validation_data=dev_dataset, epochs=epochs)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeB9ZYTpxa9J",
        "outputId": "5bce15ee-9999-4ce5-8b3e-48b12e101d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "13/13 [==============================] - 71s 812ms/step - loss: 1.8936 - accuracy: 0.2500 - val_loss: 1.7706 - val_accuracy: 0.3857\n",
            "Epoch 2/15\n",
            "13/13 [==============================] - 5s 387ms/step - loss: 1.6761 - accuracy: 0.4550 - val_loss: 1.5471 - val_accuracy: 0.6600\n",
            "Epoch 3/15\n",
            "13/13 [==============================] - 5s 364ms/step - loss: 1.4400 - accuracy: 0.7650 - val_loss: 1.2414 - val_accuracy: 0.8714\n",
            "Epoch 4/15\n",
            "13/13 [==============================] - 5s 414ms/step - loss: 1.0779 - accuracy: 0.9400 - val_loss: 0.9265 - val_accuracy: 0.9000\n",
            "Epoch 5/15\n",
            "13/13 [==============================] - 3s 281ms/step - loss: 0.7636 - accuracy: 0.9800 - val_loss: 0.7011 - val_accuracy: 0.9029\n",
            "Epoch 6/15\n",
            "13/13 [==============================] - 5s 428ms/step - loss: 0.5468 - accuracy: 0.9800 - val_loss: 0.5064 - val_accuracy: 0.9414\n",
            "Epoch 7/15\n",
            "13/13 [==============================] - 3s 277ms/step - loss: 0.3675 - accuracy: 0.9950 - val_loss: 0.3958 - val_accuracy: 0.9486\n",
            "Epoch 8/15\n",
            "13/13 [==============================] - 4s 313ms/step - loss: 0.2666 - accuracy: 0.9950 - val_loss: 0.3216 - val_accuracy: 0.9571\n",
            "Epoch 9/15\n",
            "13/13 [==============================] - 7s 560ms/step - loss: 0.1991 - accuracy: 1.0000 - val_loss: 0.2643 - val_accuracy: 0.9600\n",
            "Epoch 10/15\n",
            "13/13 [==============================] - 4s 292ms/step - loss: 0.1485 - accuracy: 1.0000 - val_loss: 0.2323 - val_accuracy: 0.9571\n",
            "Epoch 11/15\n",
            "13/13 [==============================] - 4s 289ms/step - loss: 0.1186 - accuracy: 1.0000 - val_loss: 0.2033 - val_accuracy: 0.9614\n",
            "Epoch 12/15\n",
            "13/13 [==============================] - 4s 330ms/step - loss: 0.0969 - accuracy: 1.0000 - val_loss: 0.1871 - val_accuracy: 0.9600\n",
            "Epoch 13/15\n",
            "13/13 [==============================] - 4s 296ms/step - loss: 0.0800 - accuracy: 1.0000 - val_loss: 0.1738 - val_accuracy: 0.9671\n",
            "Epoch 14/15\n",
            "13/13 [==============================] - 3s 280ms/step - loss: 0.0692 - accuracy: 1.0000 - val_loss: 0.1615 - val_accuracy: 0.9657\n",
            "Epoch 15/15\n",
            "13/13 [==============================] - 4s 327ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9657\n",
            "44/44 [==============================] - 5s 51ms/step - loss: 0.2664 - accuracy: 0.9371\n",
            "Test loss: 0.26642102003097534, Test accuracy: 0.9371428489685059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbHVK5qMxfwc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}